from pyspark.sql import SparkSession
from pyspark.sql.functions import explode
from pyspark.sql.functions import split

spark = SparkSession.builder.appName(
    "StructuredNetworkWordCount"
).getOrCreate()

# userSchema = .add("name", "string").add("salary", "integer")

# Create DataFrame representing the stream of input lines from connection to localhost:9999
lines = (
    spark.readStream.option("sep", ";").option("header", "false")
    # .schema(userSchema)
    .csv("assets/data.csv")
)

# Split the lines into words
words = lines.select(explode(split(lines.value, " ")).alias("word"))

# Generate running word count
wordCounts = words.groupBy("word").count()

# Start running the query that prints the running counts to the console
query = wordCounts.writeStream.outputMode("complete").format("console").start()

query.awaitTermination()
